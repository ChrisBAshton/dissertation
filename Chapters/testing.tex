\chapter{Testing}

SmartResolution is an open-source platform being marketed to those wanting to provide ODR services, and as such it is very important that the platform itself is thoroughly tested. Subscribers need to have confidence that the core platform works as it should, and module developers need to have confidence that the underlying system is robust enough to support their module. For these reasons, the core platform implementation was done alongside unit and integration tests throughout.

In an ideal world, the same principles would have been applied to the Maritime Collision module and to the SmartResolution website and marketplace. However, developing the core platform in a test-driven way turned out to be quite a slow, methodical process - read more in the evaluation section - and I felt that doing the same for the lesser two components was a luxury I could not afford with an impeding deadline. Therefore, this section is mostly concerned with testing the core SmartResolution software.

As previously mentioned, the project was developed in an agile way from mid-way through the design stage onwards. As such, the principles of business-driven development (BDD) and continuous integration (CI) were applied throughout the implementation. This section discusses the testing strategy that was applied during the implementation.

\section{BDD}

The project was developed in a business-driven way, as demonstrated in figure~\ref{uml:bdd}.

\begin{figure}[h!]
  \centering
    \ifimages
    \includegraphics[width=\textwidth]{bdd}
    \fi
  \caption{The BDD development process: an extension of TDD}
  \label{uml:bdd}
\end{figure}

To begin with, a Cucumber feature is selected and executed. It should fail. Now a unit test is written to describe a low-level implementation of the feature. The unit test is run and again it should fail. The next step is to write the simplest code possible to make the unit test pass.

With the test passing, now is the opportunity to take a step back and look at the code in the context of the codebase as a whole. Can anything be refactored? If so, apply the refactoring step, then run the unit tests again to ensure that everything still works as correctly.

The unit-test process outlined above is known as the ``red, green, refactor" workflow and is repeated over and over until we have enough of the feature implemented to make a step of the feature pass, and then another step, and so on, until the feature as a whole is completed and passes the integration test.

Occasionally, features were too cumbersome to implement in a BDD manner, as they required spanning multiple architectures (routing, database layers, models, controllers, and so on). Wherever following BDD wasn't appropriate, I added unit and end-to-end tests post-implementation.

\section{Test database}

Cucumber regression tests are end-to-end and use a headless browser to emulate the browser environment, clicking buttons and filling in forms, then querying the state of the HTML to validate that an element of functionality worked as expected.

To accomplish this, the driver (in our case, Poltergeist) accesses the server like a normal browser. However, our tests rely on the database having certain fixture-data and will also be making persistent changes to the database, so it's essential that the tests use a test database, rather than the production database.

Poltergeist allows you to override the HTTP headers sent with each request, so my Cucumber tests modify the User-Agent property to be either \lstinline{Poltergeist} or \lstinline{Poltergeist--clear}. Both headers inform the application that it should use the test database, but the latter provides an additional instruction that the database should be cleared and re-populated with test data before processing the request. This is demonstrated in figure~\ref{uml:headers}.

\begin{figure}[h!]
  \centering
    \ifimages
    \includegraphics[width=\textwidth]{headers}
    \fi
  \caption{Visualisation of how SmartResolution determines which database to use depending on the received HTTP headers}
  \label{uml:headers}
\end{figure}

There's nothing stopping the end-user changing the headers that they send, so they too could access SmartResolution and interact with the test environment rather than the production environment. Ultimately, this is completely harmless. The test database is cleaned every time the test suite is run, so they would not even be able to sabotage the tests. This is a tried and tested technique, and has been adopted by companies including the BBC~\cite{bbc:cucumber}.

\section{Unit Testing}

Almost every class in the system has an associated unit test. This does not apply to library classes, such as those provided by F3, since the responsibility for testing those classes resides with the third party. Unit tests verify that the publicly available methods of a class work as expected, both when accessed correctly and incorrectly.

The nature of unit tests should be that they are loaded into memory and are very quick to run. ``Depending on your platform, your testing tool should be able to run at least 100 unit tests per second."~\cite{artOfAgile} True unit tests are also entirely self-contained. According to Michael Feathers, a ``test is not a unit test if it talks to the database, it communicates across the network, it touches the file system or it can't run at the same time as any of your other unit tests.''~\cite{feathers:unitTests}

Some of the unit tests in SmartResolution are not `true' unit tests, as they rely on a database connection and the existence of specific fixture data. Additionally, some unit tests might alter, add or remove items from the database.

Towards the end of the project I refactored my tests to cut down on the number of unit tests that were tightly coupled to the database: this is discussed later on in this section. Until then, I'll discuss my testing strategy from the mindset I held at the beginning of the project, which was that code coverage is the most important metric, not test speed.

\subsection{Database transaction threads}

\subsubsection{Database transactions and PHP}

Often, we want to accomplish several things in one `transaction', for example, debiting one person's account and crediting the other person's account. If the debiting of the first person's account fails - for example, if doing so would make their account balance negative - then we don't want the other person's account to be credited. Both queries must happen, or neither must happen.

Transactions are supported in PDO (PHP Data Objects) and in F3's wrapper for PDO, where we are able to define the start and end points of a transaction. The above example could be represented in code as follows, and if either query in the transaction fails, neither would be applied:

\begin{lstlisting}[language=php]
$db->begin();
$acc1->debit(400);
$acc2->credit(400);
$db->commit();
\end{lstlisting}

\subsubsection{Issues with testing database transactions}

Between test suites, SmartResolution runs the `clear database' command to revert the test database to a known constant: an untainted database filled with predefined fixture data. This reverses any changes that may have been made to the database when running the previous test suite.

This is a useful technique in ensuring that test suites don't corrupt one another's results and that each suite works with the same set of data. However, it is slow and takes around a second on a reasonably powerful machine, becoming a problem if the script is called too often.

I made a conscious effort to keep these resets to a minimum, but a problem I faced early on in my unit tests was the following error message:

\begin{lstlisting}
PDOException: There is already an active transaction!
\end{lstlisting}

The exception refers to a situation in database transactions whereby a transaction was initiated, perhaps some queries were queued for the transaction, and then there was another request to begin a transaction. Somewhere in SmartResolution, a transaction was not being committed or rolled back.

This exception was usually raised after testing a part of my application that should (and does) raise an exception, such as trying to assign a law firm as the agent of a dispute. The expected exception was being raised and that was preventing the change (the assignment of the law firm to the dispute) from being made persistent, but subsequent attempts to begin new transactions complained that a transaction was already in progress.

Early on, this wasn't too much of an issue: I was able to run the `clear database' command between any unit tests that raised this exception, as removing and re-creating the database would naturally clear any outstanding transactions. This was a bit of a hack, but felt justified as the test and production environments are very different. In the test environment, hundreds of transactions are initiated when running the unit tests and all tests are hooking into the same database connection. In the production environment this would not happen: if an exception is raised, an error page is triggered and the user is faced with an error message relating to the exception. Once PHP delivers the error page to the user's browser, the database connection is no longer open and thus a new transaction can begin.

For several weeks, I was happy with this solution, but it did mean that my tests were quite slow to run. You can see in a Travis build around that time that it took almost a minute to run 90 unit tests\footnote{A build on 15th April, before refactor: \url{https://travis-ci.org/ChrisBAshton/smartresolution/builds/58603809}}, violating James Shore's principle that unit tests should run almost instantly.

Eventually, I decided to spend some time investigating the issue. I deduced that the transaction was not being committed because an exception was being raised before it could be committed. What I did not realise was that transactions were not being automatically rolled back when an exception was raised - it was something that needed to be handled manually in each exception.

Handling the transaction rollback in each raised exception would mean writing code like this:

\begin{minipage}{\textwidth}
\begin{lstlisting}[language=php]
$db->begin();

// several lines of code and database queries

if (// some condition) {
    $db->rollback();
    throw new Exception('Error!');
}

// more lines of code and database queries

if (// another condition) {
    $db->rollback();
    throw new Exception('Another error!');
}

$db->commit();
\end{lstlisting}
\end{minipage}

This seemed laborious, error-prone and messy. Instead, I defined a custom exception function which throws an exception, but also rolls back any existing transactions.

\begin{minipage}{\textwidth}
\begin{lstlisting}[language=php]
    public function throwException($message) {
        try {
            Database::instance()->rollback();
        } catch (Exception $PDOException) {
            // do nothing - we only wanted to roll back the transaction if one existed.
            // since one doesn't exist, there's nothing to roll back. Let's just continue and
            // throw the Exception we wanted to throw in the first place.
        }
        throw new Exception($message);
    }
\end{lstlisting}
\end{minipage}

Now that the root of the problem had been fixed, unit tests could be run without clearing the database in between each test. This led to big improvements in test speed: at this stage, I could now run 112 unit tests in 34 seconds.\footnote{A build on 17th April, after refactor: \url{https://travis-ci.org/ChrisBAshton/smartresolution/builds/58932345}} This is 3.29 unit tests per second (UTPS), compared with 1.58 UTPS before the optimisation.

I still needed to clear the database between each test \emph{suite}, as the tests would still alter the state of the database. But now, by and large, most of the tests \emph{within} those suites did not need to run on a new database connection.

\subsection{Refactoring to pure unit tests}

Earlier I described how `pure' unit tests should not query a database. An unfortunate consequence of the implementation of my original code was that database querying was essential to unit test my classes, as my models were populated by an ID representing a row in the database, rather than populated with an array of data which may or may not have come from a database.

Back in section~\ref{section:refactoringToPureModels} I discussed the late move to refactor my codebase to use only pure models, moving the database querying out of the models and into the controllers. The advantage of the refactored solution is that the models became pure: they now have no concept of a persistence layer. We could now test our models by passing arrays of hard-coded data, removing the need to retrieve from and write to a database.

Changing object constructors to take arrays rather than database record IDs meant I could cut down test times significantly. After implementing the refactored solution, my test suite performed 109 unit tests in just 12.7 seconds: a UTPS of 8.57.\footnote{A build on 22nd April, after final refactor: \url{https://travis-ci.org/ChrisBAshton/smartresolution/builds/59532839}} This information is summarised in comparison table~\ref{table:testTimes}. It is worth noting that those are the results on Travis: on my MacBook Pro, the entire unit test suite runs in under six seconds.

\begin{table}[h!]
\label{table:testTimes}
\begin{center}
\begin{tabular}{ l | r | r | r | r | r}
  Stage in project & Unit tests & Assertions & Time taken (s) & UTPS & APS \\
  \hline
  Pre-refactor & 90 & 221 & 57.70 & 1.58 & 3.83\\
  Fixed database transactions & 112 & 263 & 34.24 & 3.29 & 7.68\\
  Refactored to use only pure models & 109 & 309 & 12.72 & 8.57 & 24.29
\end{tabular}
\end{center}
\caption {Table showing the unit test times after various refactoring steps}
\end{table}

\section{Functional Testing}

The SmartResolution core software is described through a collection of Cucumber features, associated with step definitions. These features are executable and can automatically verify that the system is working as expected. As a reminder, the full list of tested features is outlined in appendix~\ref{appendix:requirements}, and the justification for the choice of Ruby and Cucumber as the BDD framework is in appendix~\ref{appendix:bdd}.

\subsection{Functional tests structure}

Every Cucumber feature has a corresponding step definition, defined in a step definition file of the same name as the feature. Each step definition defines a step in a Cucumber scenario in terms of interacting with the page, through methods provided by the Capybara acceptance test framework. Capybara requires a browser driver such as Poltergeist to request and then render the webpage. The browser can be headless (running as a background process, hidden from view) or a window browser. Poltergeist, as the name might suggest, is headless.

In cases where step definitions were beginning to rely on methods defined in other step files, I extracted the methods out as a common helper class. For example, step definitions have access to a `Session' helper class which provides methods to log in with specific credentials, or log into account types (e.g. \lstinline{Session.login_as_agent}). Figure~\ref{uml:cucumber} demonstrates the relationship between a Cucumber feature, its step definition, and the helper classes.

\begin{figure}[h!]
  \centering
    \ifimages
    \includegraphics[width=\textwidth]{cucumber}
    \fi
  \caption{Visualisation of which files are involved in defining a Cucumber feature}
  \label{uml:cucumber}
\end{figure}

Finally, there are also numerous steps which are repeated across different features, such as ``Given I am logged into an Agent account". In cases such as these, it became difficult to maintain those step definitions when it could have been buried in any of the step definition files whose features depend on it, so these were extracted to a `\_common.rb' step definition file. This is now the first place one should look when trying to find and maintain a step definition.

\subsection{Database-clearance optimisation}

The process of clearing the database involves removing the SQLite database entirely, creating a new SQLite database, executing the table-setup SQL (all performed via shell commands through PHP's \lstinline{shell_exec} function) and then seeding the database with fixture data. This fixture data is defined in \lstinline{data/fixtures/fixture_data.yml} and the mapping of YAML data to SQL tables is defined in \lstinline{data/fixtures/seed.php}. As already mentioned, this is quite a slow process and takes around a second even on a powerful machine.

I refactored my unit tests to cut down the number of `clear database' instructions, but this was an optimisation I had in mind from the start when it came to my Cucumber features. Unlike unit tests, Cucumber features test the system from the user's perspective and by their very nature require interaction with the database. The database needs clearing and re-populating between some tests to prevent the actions of one test from corrupting the other, ensuring that every test has access to the same data.

I kept the number of `clear database' instructions to a minimum in my Cucumber tests by annotating specific features with the \lstinline{@clear} tag to indicate that the database should be cleared before executing that feature. Any features without that tag are less likely to be testing something that makes persistent changes. The inclusion of the \lstinline{@clear} tag tells Poltergeist to send the \lstinline{Poltergeist--clear} header rather than just the \lstinline{Poltergeist} header.

\subsection{A note on feature style}

There is a trade-off between having a verbose Cucumber feature and a verbose step-definition. The former bogs the feature down in detail and risks devaluing it, whereas the latter can make maintenance more difficult.

I argued this same internal battle in the \emph{Developing Internet-Based Applications} assignment, discussing the pros and cons of each in some detail. The relevant extract of that report is included here in appendix~\ref{appendix:cucumber}.

My conclusion was that, though the two need to be balanced, it isn't detrimental to specify specific fields and error messages in the Cucumber feature itself. The alternative - abstracting that information away in the step definition - can make it difficult to maintain state throughout the scenario. Following this conclusion, some of the Cucumber features decided upon in the initial requirements were later reworded to fit in with this philosophy, though their purpose and relevance remained the same.

For example, this was an original `lifespan negotiation' scenario:

\begin{lstlisting}
  Scenario: Accepting a Dispute lifespan offer
    Given the other Agent has sent me a Dispute lifespan offer
    Then I should be able to accept the offer
    And the Dispute should start
\end{lstlisting}

This is how the scenario looks now that the step definitions have been implemented:

\begin{lstlisting}
  Scenario: Accepting a Dispute lifespan offer
    Given the other Agent has sent me a Dispute lifespan offer
    Then I should be able to Accept the offer
    And I should see the message 'Dispute starts in'
\end{lstlisting}

The original and amended scenarios are very similar, but the latter is a little more tied to the implementation. The `I should see the message' step definition is reused across many scenarios and means that the step definitions as a whole are simpler and cleaner, even if the Cucumber features themselves are slightly more verbose than before.

\section{Continuous Integration}

Travis CI ran all unit and functional tests on every pushed commit or pull request, automatically notifying via email if anything broke. It was an invaluable tool throughout the project and was well worth spending time configuring.

The most difficult aspect to setting up Travis was getting the PHP server running, since Travis only supports one terminal window. The PHP server had to be set up as a background process so that Travis could continue to run the other commands required to run the tests.

Travis wasn't the only tool that performed a service on every pushed commit. Just as important as ensuring that all tests continue to pass is ensuring that dependencies remain up-to-date. Gemnasium periodically checks the status of SmartResolution's dependencies and warns via the embedded SVG in the project README whether or not a newer version is available.

Dependency tracking is important, as newer versions of dependencies fix bugs and vulnerabilities discovered in older versions. By not updating SmartResolution's dependencies regularly I would risk crackers being able to exploit unpatched vulnerabilities. This is especially relevant since SmartResolution is open-source and anybody can see which version of which third-party library SmartResolution is using.

Another important thing to monitor with each commit is code quality, something that CodeClimate is trying to automate. Whenever I push a commit to SmartResolution, CodeClimate queues another scan of the codebase and informs me on a level of 1-4 whether the quality of my codebase has improved or deteriorated, using metrics such as variable name length, code duplication, the number of possible paths through a block of code, and so on. Utilising this third-party service helped fight the temptation to hack a bit of functionality in, instead encouraging good engineering practice and providing suggestions as to how the codebase could be made more maintainable.

\section{User Testing}

Whereas it is the software engineer's responsibility to ensure that the project is built right, it is the customer's responsibility to ensure that the right project is built. Therefore, user testing with the customer is critical.

As has already been highlighted, the customer for this project was extremely busy and was unable to meet more than a couple of times. This made user-testing very difficult, but I tried my best to offer alternative options.

It was good that I spent so long clarifying requirements at the beginning of the project. In the absence of regular communication and feedback, this original set of features was invaluable in ensuring that the system contained all of the required functionality.

Mid-way through the project, I deployed SmartResolution to smartresolution.org and invited all stakeholders to try out a beta version of the project, describing how to log into the system, what functionality had already been implemented, and what functionality had yet to be implemented.

Throughout development I adhered to the agile principle of regular releases, pushing the latest stable version of SmartResolution to the website on a daily basis so that the customer would be able to feel the benefit and experience a more feature-complete demo.

Later on in the project I made it even easier to demo the software, by producing a short video demonstrating the software and the maritime collision module. This is embedded on the homepage of smartresolution.org and enables users to see what the system is capable of without having to manually log in and out of demo accounts representing different user roles.

Where feedback was lacking from the customer, I turned to friends, family and the Twitter community to try out the demo and give me constructive comments to work on. These comments were fed back directly into my design, leading to the following improvements:

\begin{itemize}
\item Smaller icons. My early designs used dashboard icons around 3 times larger than the final design. I was trying to emulate clear and minimalist dashboards such as the one on DigitalOcean (see figure~\ref{screenshot:digitalOcean}) but my implementation was something consistently questioned in the feedback I received.
\item Larger font size. Bootstrap's default font was too small given the sparseness of the design as a whole. I increased the base font size; all font sizes across the site increased in size proportionally, as my CSS defined fonts in terms of \lstinline{em}.
\item Responsive improvements. It was never a requirement to make the SmartResolution software fully responsive: the fact that Bootstrap supports responsiveness by default was just an added bonus. However, some of my design decisions - such as absolutely positioning the lifespan status at the right hand side of the dispute - made for a poor experience on mobile. When this was pointed out in the feedback, I added a media query so that the positioning would only apply on devices of a minimum screen width.
\end{itemize}

\begin{figure}[h!]
  \centering
    \ifimages
    \includegraphics[width=\textwidth]{digitalocean}
    \fi
  \caption{Screenshot of the clean and simple DigitalOcean interface I wanted to emulate in the design for SmartResolution}
  \label{screenshot:digitalOcean}
\end{figure}

Feedback regarding the workflow of a dispute had to be treated differently, since the customer was the law expert and the people giving feedback were not law experts. However, their comments were collected and fed back to the law expert for trying to negotiate simpler dispute workflows.

One of the original requirements was for a formal resolution-offering process. An agent would be able to make an offer by filling in a HTML form denoting offer details, damages awarded, and so on. I was able to simplify this in light of the feedback: agents are human beings and can reach that kind of resolution through the communication option alone. They can then close the dispute successfully, with no need to officially form a structured offer that SmartResolution understands. Removing these artificial constraints made for a more user-friendly workflow and reduced the development overheads significantly.